{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mudcard\n",
    "\n",
    "- **I was struggling to determine whether a variable was continuous or ordinal such as age or gross pay**\n",
    "    - Yeah, there is no easy way to give definite answer to such questions and the answer might depend on the dataset and problem you are trying to solve\n",
    "    - If you really can't decide, develop two identical ML pipelines with one difference: treat age as continuous in one model; and as ordinal in the other model. Compare the validation scores of the two model and choos the one that gives you a better score!\n",
    "    - This is called data-driven decision making. You perform an experiment to decide which approach is better.\n",
    "- **I'm just asking about the visualization application. In the last part, you shared a link of https://pyviz.org/, where I saw the application of the dashboard, which is extremely useful (I was trying to build one during the internship). Is there any chance that we can cover this part or any other part of reporting our project during the semester?**\n",
    "    - Ufortunately we don't have time to cover visualizations in more detail but check out DATA1500 during the spring term. The whole course is about visualizations!\n",
    "- **Will visualizing data from multiple columns better demonstrate technical strength for the final project?**\n",
    "    - If you plan to show scatter matrices in your final report/presentation, then the answer is nope. It will demonatrate that you have no clear idea what you want to visuzalize :)\n",
    "        - Your figure should have a goal, a message you are trying to convey to your audience.\n",
    "        - Scatter matrices are extremely messy and bad for this purpose.\n",
    "    - if you plan to create 3D plots, I'd advise you against it. those are pretty difficult to make sense of unless they are interactive.\n",
    "- **When should we do log-transformations to bins and how exactly should we do it?**\n",
    "    - How exactly you should do it is in the lecture notes, check out the code above the figure with the log bins\n",
    "    - You should use log transformation if the quantity you want to visualize varies several orders of magnitudes. For example, salaries can be on the order of 10k USD to well above 10^6 - 10^7 USD which is 2-3 orders of magnitude.\n",
    "- **Processing the data before creating visualizations is confusing at times. Sometimes we need to create a matrix, sometimes we need to get counts, other times we just need a column. It's a bit confusing which cases we need when and why.**\n",
    "    - Not sure I understand the question. We didn't process the data yet. We just visualized 1, 2, or a couple of columns so far.\n",
    "    - Come to the office hours and talk to me or the TAs, or post on the course forum.\n",
    "- **The heatmap visualization, even when refined, feels oddly vague and hard to conceptualize. Could you show an example of when a heatmap is clearly the optimal visualization tool?**\n",
    "    - It is the optimal visualiation tool when a scatter plot fails for some reason (usually because there are too many overlaping points on the scatter plot).\n",
    "- **for the second quiz in this lecture, where do we go to find the variable type (categorical vs. continuous), i think you mentioned finding it in github but I'm not sure where to go!**\n",
    "    - the course's github repo has a text file, the dataset description\n",
    "    - but you can also just do .descibe() or .value_counts() in the notebook to quickly figure out the feature properties.\n",
    "- **In the case of one catergorical data type, when should you use a histogram and when should you use a bar plot? Or is it essentially the same and its just data-dependent?**\n",
    "    -  you always use a bar plot with categorical data, and histograms with continuous data\n",
    "    -  the main difference is that the order of the bars does not matter when you visualize categorical data; but the order of the histogram bars matter a lot! The bins are ordered!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Split iid data\n",
    "By the end of this lecture, you will be able to\n",
    "- apply basic split to iid datasets\n",
    "- apply k-fold split to iid datasets\n",
    "- apply stratified splits to imbalanced data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The supervised ML pipeline\n",
    "The goal: Use the training data (X and y) to develop a <font color='red'>model</font> which can <font color='red'>accurately</font> predict the target variable (y_new') for previously unseen data (X_new).\n",
    "\n",
    "**1. Exploratory Data Analysis (EDA)**: you need to understand your data and verify that it doesn't contain errors\n",
    "   - do as much EDA as you can!\n",
    "    \n",
    "<span style=\"background-color: #FFFF00\">**2. Split the data into different sets**: most often the sets are train, validation, and test (or holdout)</span>\n",
    "   - practitioners often make errors in this step!\n",
    "   - you can split the data randomly, based on groups, based on time, or any other non-standard way if necessary to answer your ML question\n",
    "\n",
    "**3. Preprocess the data**: ML models only work if X and Y are numbers! Some ML models additionally require each feature to have 0 mean and 1 standard deviation (standardized features)\n",
    "   - often the original features you get contain strings (for example a gender feature would contain 'male', 'female', 'non-binary', 'unknown') which needs to transformed into numbers\n",
    "   - often the features are not standardized (e.g., age is between 0 and 100) but it needs to be standardized\n",
    "    \n",
    "**4. Choose an evaluation metric**: depends on the priorities of the stakeholders\n",
    "   - often requires quite a bit of thinking and ethical considerations\n",
    "     \n",
    "**5. Choose one or more ML techniques**: it is highly recommended that you try multiple models\n",
    "   - start with simple models like linear or logistic regression\n",
    "   - try also more complex models like nearest neighbors, support vector machines, random forest, etc.\n",
    "    \n",
    "**6. Tune the hyperparameters of your ML models (aka cross-validation)**\n",
    "   - ML techniques have hyperparameters that you need to optimize to achieve best performance\n",
    "   - for each ML model, decide which parameters to tune and what values to try\n",
    "   - loop through each parameter combination\n",
    "       - train one model for each parameter combination\n",
    "       - evaluate how well the model performs on the validation set\n",
    "   - take the parameter combo that gives the best validation score\n",
    "   - evaluate that model on the test set to report how well the model is expected to perform on previously unseen data\n",
    "    \n",
    "**7. Interpret your model**: black boxes are often not useful\n",
    "   - check if your model uses features that make sense (excellent tool for debugging)\n",
    "   - often model predictions are not enough, you need to be able to explain how the model arrived to a particular prediction (e.g., in health care)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why do we split the data?\n",
    "- we want to find the best hyper-parameters of our ML algorithms\n",
    "   - fit models to training data\n",
    "   - evaluate each model on validation set\n",
    "   - we find hyper-parameter values that optimize the validation score\n",
    "- we want to know how the model will perform on previously unseen data\n",
    "   - apply our final model on the test set\n",
    "   \n",
    "### We need to split the data into three parts!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Recap from the second lecture\n",
    "\n",
    "- **the learner's input**\n",
    "    - Domain set $\\mathcal{X}$ - a set of objects we wish to label.\n",
    "    - Label set $\\mathcal{Y}$ - a set of possible labels. \n",
    "    - Training data $S = ((x_1, y_1),...,(x_m,y_m))$ - a finite sequence of pairs from $\\mathcal{X}$, $\\mathcal{Y}$. This is what the learner has access to.\n",
    "        - $X = (x_1,...,x_m)$ is the feature matrix which is usually a 2D matrix, and $Y = (y_1,...,y_m)$ is the target variable which is a vector.\n",
    "        \n",
    "- let's denote the probability distribution over $\\mathcal{X}$ by $D$.\n",
    "- let's assume there is some correct labeling function $f : \\mathcal{X} \\rightarrow \\mathcal{Y}$. \n",
    "- **a training example is then generated by sampling $x_i$ from $D$, and the label $y_i$ is generated using $f$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## I.I.D. assumption\n",
    "\n",
    "- **the i.i.d. assumption**: the examples in the training set are independently and identically distributed according to $D$\n",
    "    - every $x_i$ is freshly sampled from $D$ and then labelled by $f$\n",
    "    - that is, $x_i$ and $y_i$ are picked independently of the other instances\n",
    "    - $S$ is a window through which the learner gets partial info about $D$ and the labeling function $f$\n",
    "    - the larger the sample gets, the more likely it is to reflect more accurately $D$ and $f$\n",
    "    \n",
    "\n",
    "- examples of not iid data:\n",
    "   - data generated by time-dependent processes\n",
    "   - data has group structure (samples collected from e.g., different subjects, experiments, measurement devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='lightgray'>Split iid data</font>\n",
    "<font color='lightgray'>By the end of this lecture, you will be able to</font>\n",
    "- **apply basic split to iid datasets**\n",
    "- <font color='lightgray'>apply k-fold split to iid datasets</font>\n",
    "- <font color='lightgray'>apply stratified splits to imbalanced data</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Splitting strategies for iid data: basic approach\n",
    "- 60% train, 20% validation, 20% test for small datasets\n",
    "- 98% train, 1% validation, 1% test for large datasets\n",
    "    - if you have 1 million points, you still have 10000 points in validation and test which is plenty to assess model performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Let's work with the adult data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         <=50K.\n",
      "1         <=50K.\n",
      "2          >50K.\n",
      "3          >50K.\n",
      "4         <=50K.\n",
      "          ...   \n",
      "16276     <=50K.\n",
      "16277     <=50K.\n",
      "16278     <=50K.\n",
      "16279     <=50K.\n",
      "16280      >50K.\n",
      "Name: gross-income, Length: 16281, dtype: object\n",
      "   age   workclass  fnlwgt      education  education-num       marital-status  \\\n",
      "0   25     Private  226802           11th              7        Never-married   \n",
      "1   38     Private   89814        HS-grad              9   Married-civ-spouse   \n",
      "2   28   Local-gov  336951     Assoc-acdm             12   Married-civ-spouse   \n",
      "3   44     Private  160323   Some-college             10   Married-civ-spouse   \n",
      "4   18           ?  103497   Some-college             10        Never-married   \n",
      "\n",
      "           occupation relationship    race      sex  capital-gain  \\\n",
      "0   Machine-op-inspct    Own-child   Black     Male             0   \n",
      "1     Farming-fishing      Husband   White     Male             0   \n",
      "2     Protective-serv      Husband   White     Male             0   \n",
      "3   Machine-op-inspct      Husband   Black     Male          7688   \n",
      "4                   ?    Own-child   White   Female             0   \n",
      "\n",
      "   capital-loss  hours-per-week  native-country  \n",
      "0             0              40   United-States  \n",
      "1             0              50   United-States  \n",
      "2             0              40   United-States  \n",
      "3             0              40   United-States  \n",
      "4             0              30   United-States  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "df = pd.read_csv('data/adult_test.csv')\n",
    "\n",
    "# let's separate the feature matrix X, and target variable y\n",
    "y = df['gross-income'] # remember, we want to predict who earns more than 50k or less than 50k\n",
    "X = df.loc[:, df.columns != 'gross-income'] # all other columns are features\n",
    "print(y)\n",
    "print(X.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train_test_split in module sklearn.model_selection._split:\n",
      "\n",
      "train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)\n",
      "    Split arrays or matrices into random train and test subsets.\n",
      "\n",
      "    Quick utility that wraps input validation,\n",
      "    ``next(ShuffleSplit().split(X, y))``, and application to input data\n",
      "    into a single call for splitting (and optionally subsampling) data into a\n",
      "    one-liner.\n",
      "\n",
      "    Read more in the :ref:`User Guide <cross_validation>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    *arrays : sequence of indexables with same length / shape[0]\n",
      "        Allowed inputs are lists, numpy arrays, scipy-sparse\n",
      "        matrices or pandas dataframes.\n",
      "\n",
      "    test_size : float or int, default=None\n",
      "        If float, should be between 0.0 and 1.0 and represent the proportion\n",
      "        of the dataset to include in the test split. If int, represents the\n",
      "        absolute number of test samples. If None, the value is set to the\n",
      "        complement of the train size. If ``train_size`` is also None, it will\n",
      "        be set to 0.25.\n",
      "\n",
      "    train_size : float or int, default=None\n",
      "        If float, should be between 0.0 and 1.0 and represent the\n",
      "        proportion of the dataset to include in the train split. If\n",
      "        int, represents the absolute number of train samples. If None,\n",
      "        the value is automatically set to the complement of the test size.\n",
      "\n",
      "    random_state : int, RandomState instance or None, default=None\n",
      "        Controls the shuffling applied to the data before applying the split.\n",
      "        Pass an int for reproducible output across multiple function calls.\n",
      "        See :term:`Glossary <random_state>`.\n",
      "\n",
      "    shuffle : bool, default=True\n",
      "        Whether or not to shuffle the data before splitting. If shuffle=False\n",
      "        then stratify must be None.\n",
      "\n",
      "    stratify : array-like, default=None\n",
      "        If not None, data is split in a stratified fashion, using this as\n",
      "        the class labels.\n",
      "        Read more in the :ref:`User Guide <stratification>`.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    splitting : list, length=2 * len(arrays)\n",
      "        List containing train-test split of inputs.\n",
      "\n",
      "        .. versionadded:: 0.16\n",
      "            If the input is sparse, the output will be a\n",
      "            ``scipy.sparse.csr_matrix``. Else, output type is the same as the\n",
      "            input type.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> import numpy as np\n",
      "    >>> from sklearn.model_selection import train_test_split\n",
      "    >>> X, y = np.arange(10).reshape((5, 2)), range(5)\n",
      "    >>> X\n",
      "    array([[0, 1],\n",
      "           [2, 3],\n",
      "           [4, 5],\n",
      "           [6, 7],\n",
      "           [8, 9]])\n",
      "    >>> list(y)\n",
      "    [0, 1, 2, 3, 4]\n",
      "\n",
      "    >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "    ...     X, y, test_size=0.33, random_state=42)\n",
      "    ...\n",
      "    >>> X_train\n",
      "    array([[4, 5],\n",
      "           [0, 1],\n",
      "           [6, 7]])\n",
      "    >>> y_train\n",
      "    [2, 0, 3]\n",
      "    >>> X_test\n",
      "    array([[2, 3],\n",
      "           [8, 9]])\n",
      "    >>> y_test\n",
      "    [1, 4]\n",
      "\n",
      "    >>> train_test_split(y, shuffle=False)\n",
      "    [[0, 1, 2], [3, 4]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(train_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: (9768, 14) (9768,)\n",
      "(6513, 14) (6513,)\n",
      "validation set: (3256, 14) (3256,)\n",
      "test set: (3257, 14) (3257,)\n",
      "       age          workclass  fnlwgt      education  education-num  \\\n",
      "4050    22            Private  335950        HS-grad              9   \n",
      "11446   29            Private   78261        HS-grad              9   \n",
      "12427   74   Self-emp-not-inc  160009     Assoc-acdm             12   \n",
      "5702    39       Self-emp-inc   31709   Some-college             10   \n",
      "13058   50            Private  144084        HS-grad              9   \n",
      "\n",
      "            marital-status        occupation    relationship    race      sex  \\\n",
      "4050         Never-married     Other-service   Not-in-family   Black     Male   \n",
      "11446            Separated   Protective-serv   Not-in-family   White     Male   \n",
      "12427   Married-civ-spouse   Exec-managerial         Husband   White     Male   \n",
      "5702    Married-civ-spouse      Adm-clerical            Wife   White   Female   \n",
      "13058            Separated             Sales       Unmarried   White   Female   \n",
      "\n",
      "       capital-gain  capital-loss  hours-per-week  native-country  \n",
      "4050              0             0              70   United-States  \n",
      "11446             0             0              55   United-States  \n",
      "12427             0             0              30   United-States  \n",
      "5702              0             0              20   United-States  \n",
      "13058             0             0              55   United-States  \n"
     ]
    }
   ],
   "source": [
    "random_state = 42\n",
    "\n",
    "# first split to separate out the training set\n",
    "X_train, X_other, y_train, y_other = train_test_split(X,y,\\\n",
    "                    train_size = 0.6,random_state = random_state)\n",
    "print('training set:',X_train.shape, y_train.shape) # 60% of points are in train\n",
    "print(X_other.shape, y_other.shape) # 40% of points are in other\n",
    "\n",
    "# second split to separate out the validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_other,y_other,\\\n",
    "                    train_size = 0.5,random_state = random_state)\n",
    "print('validation set:',X_val.shape, y_val.shape) # 20% of points are in validation\n",
    "print('test set:',X_test.shape, y_test.shape) # 20% of points are in test\n",
    "\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomness due to splitting\n",
    "- the model performance, validation and test scores will change depending on which points are in train, val, test\n",
    "    - inherent randomness or uncertainty of the ML pipeline\n",
    "- change the random state a couple of times and repeat the whole ML pipeline to assess how much the random splitting affects your test score\n",
    "    - you would expect a similar uncertainty when the model is deployed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 1\n",
    "\n",
    "What's the second train_test_split line if you want to end up with 60-20-20 in train-val-test? Print out the sizes of X_train, X_val, X_test to verify!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_other, X_test, y_other, y_test = train_test_split(X,y,\\\n",
    "                    train_size = 0.8,random_state=random_state)\n",
    "# add your line below and choose the correct solution from canvas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='lightgray'>Split iid data</font>\n",
    "<font color='lightgray'>By the end of this lecture, you will be able to</font>\n",
    "- <font color='lightgray'>apply basic split to iid datasets</font>\n",
    "- **apply k-fold split to iid datasets**\n",
    "- <font color='lightgray'>apply stratified splits to imbalanced data</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other splitting strategy for iid data: k-fold splitting\n",
    "\n",
    "<center><img src=\"figures/grid_search_cross_validation.png\" width=\"600\"></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class KFold in module sklearn.model_selection._split:\n",
      "\n",
      "class KFold(_UnsupportedGroupCVMixin, _BaseKFold)\n",
      " |  KFold(n_splits=5, *, shuffle=False, random_state=None)\n",
      " |\n",
      " |  K-Fold cross-validator.\n",
      " |\n",
      " |  Provides train/test indices to split data in train/test sets. Split\n",
      " |  dataset into k consecutive folds (without shuffling by default).\n",
      " |\n",
      " |  Each fold is then used once as a validation while the k - 1 remaining\n",
      " |  folds form the training set.\n",
      " |\n",
      " |  Read more in the :ref:`User Guide <k_fold>`.\n",
      " |\n",
      " |  For visualisation of cross-validation behaviour and\n",
      " |  comparison between common scikit-learn split methods\n",
      " |  refer to :ref:`sphx_glr_auto_examples_model_selection_plot_cv_indices.py`\n",
      " |\n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_splits : int, default=5\n",
      " |      Number of folds. Must be at least 2.\n",
      " |\n",
      " |      .. versionchanged:: 0.22\n",
      " |          ``n_splits`` default value changed from 3 to 5.\n",
      " |\n",
      " |  shuffle : bool, default=False\n",
      " |      Whether to shuffle the data before splitting into batches.\n",
      " |      Note that the samples within each split will not be shuffled.\n",
      " |\n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      When `shuffle` is True, `random_state` affects the ordering of the\n",
      " |      indices, which controls the randomness of each fold. Otherwise, this\n",
      " |      parameter has no effect.\n",
      " |      Pass an int for reproducible output across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |\n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.model_selection import KFold\n",
      " |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
      " |  >>> y = np.array([1, 2, 3, 4])\n",
      " |  >>> kf = KFold(n_splits=2)\n",
      " |  >>> kf.get_n_splits(X)\n",
      " |  2\n",
      " |  >>> print(kf)\n",
      " |  KFold(n_splits=2, random_state=None, shuffle=False)\n",
      " |  >>> for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
      " |  ...     print(f\"Fold {i}:\")\n",
      " |  ...     print(f\"  Train: index={train_index}\")\n",
      " |  ...     print(f\"  Test:  index={test_index}\")\n",
      " |  Fold 0:\n",
      " |    Train: index=[2 3]\n",
      " |    Test:  index=[0 1]\n",
      " |  Fold 1:\n",
      " |    Train: index=[0 1]\n",
      " |    Test:  index=[2 3]\n",
      " |\n",
      " |  Notes\n",
      " |  -----\n",
      " |  The first ``n_samples % n_splits`` folds have size\n",
      " |  ``n_samples // n_splits + 1``, other folds have size\n",
      " |  ``n_samples // n_splits``, where ``n_samples`` is the number of samples.\n",
      " |\n",
      " |  Randomized CV splitters may return different results for each call of\n",
      " |  split. You can make the results identical by setting `random_state`\n",
      " |  to an integer.\n",
      " |\n",
      " |  See Also\n",
      " |  --------\n",
      " |  StratifiedKFold : Takes class information into account to avoid building\n",
      " |      folds with imbalanced class distributions (for binary or multiclass\n",
      " |      classification tasks).\n",
      " |\n",
      " |  GroupKFold : K-fold iterator variant with non-overlapping groups.\n",
      " |\n",
      " |  RepeatedKFold : Repeats K-Fold n times.\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      KFold\n",
      " |      _UnsupportedGroupCVMixin\n",
      " |      _BaseKFold\n",
      " |      BaseCrossValidator\n",
      " |      sklearn.utils._metadata_requests._MetadataRequester\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, n_splits=5, *, shuffle=False, random_state=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __abstractmethods__ = frozenset()\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _UnsupportedGroupCVMixin:\n",
      " |\n",
      " |  split(self, X, y=None, groups=None)\n",
      " |      Generate indices to split data into training and test set.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Training data, where `n_samples` is the number of samples\n",
      " |          and `n_features` is the number of features.\n",
      " |\n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          The target variable for supervised learning problems.\n",
      " |\n",
      " |      groups : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |\n",
      " |      Yields\n",
      " |      ------\n",
      " |      train : ndarray\n",
      " |          The training set indices for that split.\n",
      " |\n",
      " |      test : ndarray\n",
      " |          The testing set indices for that split.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from _UnsupportedGroupCVMixin:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _BaseKFold:\n",
      " |\n",
      " |  get_n_splits(self, X=None, y=None, groups=None)\n",
      " |      Returns the number of splitting iterations in the cross-validator.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |\n",
      " |      y : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |\n",
      " |      groups : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      n_splits : int\n",
      " |          Returns the number of splitting iterations in the cross-validator.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseCrossValidator:\n",
      " |\n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |\n",
      " |  get_metadata_routing(self)\n",
      " |      Get metadata routing of this object.\n",
      " |\n",
      " |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      routing : MetadataRequest\n",
      " |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      " |          routing information.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |\n",
      " |  __init_subclass__(**kwargs)\n",
      " |      Set the ``set_{method}_request`` methods.\n",
      " |\n",
      " |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      " |      looks for the information available in the set default values which are\n",
      " |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      " |      from method signatures.\n",
      " |\n",
      " |      The ``__metadata_request__*`` class attributes are used when a method\n",
      " |      does not explicitly accept a metadata through its arguments or if the\n",
      " |      developer would like to specify a request value for those metadata\n",
      " |      which are different from the default ``None``.\n",
      " |\n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "help(KFold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13024, 14) (13024,)\n",
      "test set: (3257, 14) (3257,)\n",
      "   training set: (10419, 14) (10419,)\n",
      "   validation set: (2605, 14) (2605,)\n",
      "       age          workclass      education\n",
      "9850    59            Private   Some-college\n",
      "103     58   Self-emp-not-inc            9th\n",
      "1383    45            Private        HS-grad\n",
      "11034   49   Self-emp-not-inc      Bachelors\n",
      "14876   59   Self-emp-not-inc      Bachelors\n",
      "   training set: (10419, 14) (10419,)\n",
      "   validation set: (2605, 14) (2605,)\n",
      "       age     workclass      education\n",
      "13384   60   Federal-gov      Bachelors\n",
      "8471    20       Private        HS-grad\n",
      "13406   21             ?   Some-college\n",
      "13394   35       Private        HS-grad\n",
      "15123   38       Private   Some-college\n",
      "   training set: (10419, 14) (10419,)\n",
      "   validation set: (2605, 14) (2605,)\n",
      "       age     workclass      education\n",
      "647     60             ?      Bachelors\n",
      "9314    26       Private   Some-college\n",
      "14499   52       Private        HS-grad\n",
      "7332    53   Federal-gov     Assoc-acdm\n",
      "12523   21       Private           10th\n",
      "   training set: (10419, 14) (10419,)\n",
      "   validation set: (2605, 14) (2605,)\n",
      "       age workclass      education\n",
      "5294    53   Private        HS-grad\n",
      "3481    41   Private        HS-grad\n",
      "7671    49   Private   Some-college\n",
      "11055   39   Private      Bachelors\n",
      "12751   18         ?           12th\n",
      "   training set: (10420, 14) (10420,)\n",
      "   validation set: (2604, 14) (2604,)\n",
      "       age      workclass     education\n",
      "4265    23              ?          10th\n",
      "5290    23        Private       HS-grad\n",
      "1157    56   Self-emp-inc   Prof-school\n",
      "12344   18        Private          11th\n",
      "13683   55        Private       HS-grad\n"
     ]
    }
   ],
   "source": [
    "random_state =42\n",
    "\n",
    "# first split to separate out the test set\n",
    "X_other, X_test, y_other, y_test = train_test_split(X,y,test_size = 0.2,random_state=random_state)\n",
    "print(X_other.shape,y_other.shape)\n",
    "print('test set:',X_test.shape,y_test.shape)\n",
    "\n",
    "# do KFold split on other\n",
    "kf = KFold(n_splits=5,shuffle=True,random_state=random_state)\n",
    "for train_index, val_index in kf.split(X_other,y_other):\n",
    "    X_train = X_other.iloc[train_index]\n",
    "    y_train = y_other.iloc[train_index]\n",
    "    X_val = X_other.iloc[val_index]\n",
    "    y_val = y_other.iloc[val_index]\n",
    "    print('   training set:',X_train.shape, y_train.shape) \n",
    "    print('   validation set:',X_val.shape, y_val.shape) \n",
    "    # the validation set contains different points in each iteration\n",
    "    print(X_val[['age','workclass','education']].head())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How many splits should I create?\n",
    "- tough question, 3-5 is most common\n",
    "- if you do n splits, n models will be trained, so the larger the n, the most computationally intensive it will be to train the models\n",
    "- KFold is usually better suited to small datasets\n",
    "- KFold is good to estimate uncertainty due to random splitting of train and val, but it is not perfect\n",
    "    - the test set remains the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why shuffling iid data is important?\n",
    "- by default, data is not shuffled by Kfold which can introduce errors!\n",
    "<center><img src=\"figures/kfold.png\" width=\"600\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Quiz 2\n",
    "Given the labels below, what are the balances of each class?\n",
    "\n",
    "y = [0,0,0,2,2,0,0,2,0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='lightgray'>Split iid data</font>\n",
    "<font color='lightgray'>By the end of this lecture, you will be able to</font>\n",
    "- <font color='lightgray'>apply basic split to iid datasets</font>\n",
    "- <font color='lightgray'>apply k-fold split to iid datasets</font>\n",
    "- **apply stratified splits to imbalanced data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalanced data\n",
    "- imbalanced data: only a small fraction of the points are in one of the classes, usually ~5% or less but there is no hard limit here\n",
    "- examples:\n",
    "    - people visit a bank's website. do they sign up for a new credit card?\n",
    "        - most customers just browse and leave the page\n",
    "        - usually 1% or less of the customers get a credit card (class 1), the rest leaves the page without signing up (class 0).\n",
    "    - fraud detection\n",
    "        - only a tiny fraction of credit card payments are fraudulent\n",
    "    - rare disease diagnosis\n",
    "- the issue with imbalanced data:\n",
    "    - if you apply train_test_split or KFold, you might not have class 1 points in one of your sets by chance\n",
    "    - this is what we need to fix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: stratified splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y\n",
      "0    990\n",
      "1     10\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('data/imbalanced_data.csv')\n",
    "\n",
    "X = df[['feature1','feature2']]\n",
    "y = df['y']\n",
    "\n",
    "print(y.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**balance without stratification:**\n",
      "(array([0, 1]), array([597,   3]))\n",
      "(array([0, 1]), array([197,   3]))\n",
      "(array([0, 1]), array([196,   4]))\n",
      "**balance with stratification:**\n",
      "(array([0, 1]), array([594,   6]))\n",
      "(array([0, 1]), array([198,   2]))\n",
      "(array([0, 1]), array([198,   2]))\n"
     ]
    }
   ],
   "source": [
    "# 4 and 10\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "X_train, X_other, y_train, y_other = train_test_split(X,y,train_size = 0.6,random_state=random_state)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_other,y_other,train_size = 0.5,random_state=random_state)\n",
    "\n",
    "print('**balance without stratification:**')\n",
    "# a variation on the order of 1% which would be too much for imbalanced data!\n",
    "print(np.unique(y_train,return_counts=True))\n",
    "print(np.unique(y_val,return_counts=True))\n",
    "print(np.unique(y_test,return_counts=True))\n",
    "\n",
    "X_train, X_other, y_train, y_other = train_test_split(X,y,train_size = 0.6,stratify=y,random_state=random_state)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_other,y_other,train_size = 0.5,stratify=y_other,random_state=random_state)\n",
    "print('**balance with stratification:**')\n",
    "# very little variation (in the 4th decimal point only) which is important if the problem is imbalanced\n",
    "print(np.unique(y_train,return_counts=True))\n",
    "print(np.unique(y_val,return_counts=True))\n",
    "print(np.unique(y_test,return_counts=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Stratified folds\n",
    "<center><img src=\"figures/stratified_kfold.png\" width=\"600\"></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class StratifiedKFold in module sklearn.model_selection._split:\n",
      "\n",
      "class StratifiedKFold(_BaseKFold)\n",
      " |  StratifiedKFold(n_splits=5, *, shuffle=False, random_state=None)\n",
      " |\n",
      " |  Stratified K-Fold cross-validator.\n",
      " |\n",
      " |  Provides train/test indices to split data in train/test sets.\n",
      " |\n",
      " |  This cross-validation object is a variation of KFold that returns\n",
      " |  stratified folds. The folds are made by preserving the percentage of\n",
      " |  samples for each class.\n",
      " |\n",
      " |  Read more in the :ref:`User Guide <stratified_k_fold>`.\n",
      " |\n",
      " |  For visualisation of cross-validation behaviour and\n",
      " |  comparison between common scikit-learn split methods\n",
      " |  refer to :ref:`sphx_glr_auto_examples_model_selection_plot_cv_indices.py`\n",
      " |\n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_splits : int, default=5\n",
      " |      Number of folds. Must be at least 2.\n",
      " |\n",
      " |      .. versionchanged:: 0.22\n",
      " |          ``n_splits`` default value changed from 3 to 5.\n",
      " |\n",
      " |  shuffle : bool, default=False\n",
      " |      Whether to shuffle each class's samples before splitting into batches.\n",
      " |      Note that the samples within each split will not be shuffled.\n",
      " |\n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      When `shuffle` is True, `random_state` affects the ordering of the\n",
      " |      indices, which controls the randomness of each fold for each class.\n",
      " |      Otherwise, leave `random_state` as `None`.\n",
      " |      Pass an int for reproducible output across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |\n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.model_selection import StratifiedKFold\n",
      " |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
      " |  >>> y = np.array([0, 0, 1, 1])\n",
      " |  >>> skf = StratifiedKFold(n_splits=2)\n",
      " |  >>> skf.get_n_splits(X, y)\n",
      " |  2\n",
      " |  >>> print(skf)\n",
      " |  StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\n",
      " |  >>> for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
      " |  ...     print(f\"Fold {i}:\")\n",
      " |  ...     print(f\"  Train: index={train_index}\")\n",
      " |  ...     print(f\"  Test:  index={test_index}\")\n",
      " |  Fold 0:\n",
      " |    Train: index=[1 3]\n",
      " |    Test:  index=[0 2]\n",
      " |  Fold 1:\n",
      " |    Train: index=[0 2]\n",
      " |    Test:  index=[1 3]\n",
      " |\n",
      " |  Notes\n",
      " |  -----\n",
      " |  The implementation is designed to:\n",
      " |\n",
      " |  * Generate test sets such that all contain the same distribution of\n",
      " |    classes, or as close as possible.\n",
      " |  * Be invariant to class label: relabelling ``y = [\"Happy\", \"Sad\"]`` to\n",
      " |    ``y = [1, 0]`` should not change the indices generated.\n",
      " |  * Preserve order dependencies in the dataset ordering, when\n",
      " |    ``shuffle=False``: all samples from class k in some test set were\n",
      " |    contiguous in y, or separated in y by samples from classes other than k.\n",
      " |  * Generate test sets where the smallest and largest differ by at most one\n",
      " |    sample.\n",
      " |\n",
      " |  .. versionchanged:: 0.22\n",
      " |      The previous implementation did not follow the last constraint.\n",
      " |\n",
      " |  See Also\n",
      " |  --------\n",
      " |  RepeatedStratifiedKFold : Repeats Stratified K-Fold n times.\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      StratifiedKFold\n",
      " |      _BaseKFold\n",
      " |      BaseCrossValidator\n",
      " |      sklearn.utils._metadata_requests._MetadataRequester\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, n_splits=5, *, shuffle=False, random_state=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  split(self, X, y, groups=None)\n",
      " |      Generate indices to split data into training and test set.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Training data, where `n_samples` is the number of samples\n",
      " |          and `n_features` is the number of features.\n",
      " |\n",
      " |          Note that providing ``y`` is sufficient to generate the splits and\n",
      " |          hence ``np.zeros(n_samples)`` may be used as a placeholder for\n",
      " |          ``X`` instead of actual training data.\n",
      " |\n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          The target variable for supervised learning problems.\n",
      " |          Stratification is done based on the y labels.\n",
      " |\n",
      " |      groups : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |\n",
      " |      Yields\n",
      " |      ------\n",
      " |      train : ndarray\n",
      " |          The training set indices for that split.\n",
      " |\n",
      " |      test : ndarray\n",
      " |          The testing set indices for that split.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      Randomized CV splitters may return different results for each call of\n",
      " |      split. You can make the results identical by setting `random_state`\n",
      " |      to an integer.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __abstractmethods__ = frozenset()\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _BaseKFold:\n",
      " |\n",
      " |  get_n_splits(self, X=None, y=None, groups=None)\n",
      " |      Returns the number of splitting iterations in the cross-validator.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |\n",
      " |      y : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |\n",
      " |      groups : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      n_splits : int\n",
      " |          Returns the number of splitting iterations in the cross-validator.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseCrossValidator:\n",
      " |\n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |\n",
      " |  get_metadata_routing(self)\n",
      " |      Get metadata routing of this object.\n",
      " |\n",
      " |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      routing : MetadataRequest\n",
      " |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      " |          routing information.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |\n",
      " |  __init_subclass__(**kwargs)\n",
      " |      Set the ``set_{method}_request`` methods.\n",
      " |\n",
      " |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      " |      looks for the information available in the set default values which are\n",
      " |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      " |      from method signatures.\n",
      " |\n",
      " |      The ``__metadata_request__*`` class attributes are used when a method\n",
      " |      does not explicitly accept a metadata through its arguments or if the\n",
      " |      developer would like to specify a request value for those metadata\n",
      " |      which are different from the default ``None``.\n",
      " |\n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "help(StratifiedKFold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test balance: (array([0, 1]), array([198,   2]))\n",
      "new fold\n",
      "(array([0, 1]), array([596,   4]))\n",
      "(array([0, 1]), array([196,   4]))\n",
      "new fold\n",
      "(array([0, 1]), array([593,   7]))\n",
      "(array([0, 1]), array([199,   1]))\n",
      "new fold\n",
      "(array([0, 1]), array([592,   8]))\n",
      "(array([0]), array([200]))\n",
      "new fold\n",
      "(array([0, 1]), array([595,   5]))\n",
      "(array([0, 1]), array([197,   3]))\n"
     ]
    }
   ],
   "source": [
    "# what we did before: variance in balance on the order of 1%\n",
    "random_state = 2\n",
    "\n",
    "X_other, X_test, y_other, y_test = train_test_split(X,y,test_size = 0.2,random_state=random_state)\n",
    "print('test balance:',np.unique(y_test,return_counts=True))\n",
    "\n",
    "# do KFold split on other\n",
    "kf = KFold(n_splits=4,shuffle=True,random_state=random_state)\n",
    "for train_index, val_index in kf.split(X_other,y_other):\n",
    "    print('new fold')\n",
    "    X_train = X_other.iloc[train_index]\n",
    "    y_train = y_other.iloc[train_index]\n",
    "    X_val = X_other.iloc[val_index]\n",
    "    y_val = y_other.iloc[val_index]\n",
    "    print(np.unique(y_train,return_counts=True))\n",
    "    print(np.unique(y_val,return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test balance: (array([0, 1]), array([198,   2]))\n",
      "new fold\n",
      "(array([0, 1]), array([594,   6]))\n",
      "(array([0, 1]), array([198,   2]))\n",
      "new fold\n",
      "(array([0, 1]), array([594,   6]))\n",
      "(array([0, 1]), array([198,   2]))\n",
      "new fold\n",
      "(array([0, 1]), array([594,   6]))\n",
      "(array([0, 1]), array([198,   2]))\n",
      "new fold\n",
      "(array([0, 1]), array([594,   6]))\n",
      "(array([0, 1]), array([198,   2]))\n"
     ]
    }
   ],
   "source": [
    "# stratified K Fold: variation in balance is very small (4th decimal point)\n",
    "random_state = 42\n",
    "\n",
    "# stratified train-test split\n",
    "X_other, X_test, y_other, y_test = train_test_split(X,y,test_size = 0.2,stratify=y,random_state=random_state)\n",
    "print('test balance:',np.unique(y_test,return_counts=True))\n",
    "\n",
    "# do StratifiedKFold split on other\n",
    "kf = StratifiedKFold(n_splits=4,shuffle=True,random_state=random_state)\n",
    "for train_index, val_index in kf.split(X_other,y_other):\n",
    "    print('new fold')\n",
    "    X_train = X_other.iloc[train_index]\n",
    "    y_train = y_other.iloc[train_index]\n",
    "    X_val = X_other.iloc[val_index]\n",
    "    y_val = y_other.iloc[val_index]\n",
    "    print(np.unique(y_train,return_counts=True))\n",
    "    print(np.unique(y_val,return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mudcard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
